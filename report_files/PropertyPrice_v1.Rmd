---
title: "Property Price Project Report"
author: "Raúl Galindo Martínez"
date: "12/13/2019"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
```

```{r no_scientific_notation, include=FALSE}
# Preventing scientific notation  
options(scipen=999)
```

## Table of Contents

1. Introduction/Overview/Executive Summary
   + Downloading data  
   
2. Methods/Analysis
   + Data Observation
   + Data Wrangling
   + Data Preprocessing
   + Splitting Data
   + Data Visualization
   + Applying Machine Learning Techniques
     * Last Square Estimate (LSE)
     * k-nearest neighbors
     * gamLoess
     * Regression Tree - rpart
     * Regression Tree - randomForest  
     
3. Results  

4. Conclusion

5. RStudio Version

\newpage
## 1. Introduction/Overview/Executive Summary
As part of the Data Science course, We have been tasked with a data science project which applies machine learning techniques. For this project, we can use a public dataset.

I decided to build a property price recommendation system because I'm right now interested in buying a new house. After spending some time getting familiar with websites that offer free datasets I chose  [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction). This dataset contains house sale prices for King County, which includes Seattle. It has great usability. This dataset is a single file called kc_house_data.csv. There are 21,613 observations (rows). Each line represents one house sold in King County.

We will train some machine learning algorithms using the inputs in the training set to predict property prices in the test set. The property price predictions will be compared to the true prices in the validation set using RMSE.

RMSE = $\sqrt{\frac{1}{N}\displaystyle\sum_{i=1}^{N}{\Big({\hat{Y}_{i} -Y_{i}}\Big)^2}}$ Where _N_ $\equiv$ Number of observations

We will start with a linear regression algorithm as a baseline, then we will apply more complex algorithms to try to beat it

The goal is to predict the price of housing based on the dataset.


### Downloading data
```{r required_packages, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(anytime)) install.packages("anytime", repos = "http://cran.us.r-project.org")
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(GGally)) install.packages("GGally", repos = "http://cran.us.r-project.org")
if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")

library(lubridate)
library(psych)
library(corrplot)
library(broom)

```

```{r download_data}
# Reading dataset from a csv file 
data <- read.csv("../data/kc_house_data.csv")

# feature names
names(data)
```

#### Dataset description
Feature | Description
--- | --------------
 id            | Unique ID for each observation (home sold)
 date          | Date of the home sale
 price         | Price of each home sold
 bedrooms      | Number of bedrooms
 bathrooms     | Number of bathrooms, where .5 accounts for a room with a toilet but no shower
 sqft_living   | Square footage of the apartments interior living space
 sqft_lot      | Square footage of the land space
 floors        | Number of floors
 waterfront    | A dummy variable for whether the apartment was overlooking the waterfront or not
 view          | An index from 0 to 4 of how good the view of the property was
 condition     | An index from 1 to 5 on the condition of the apartment
 grade         | An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design
 sqft_above    | The square footage of the interior housing space that is above ground level
 sqft_basement | The square footage of the interior housing space that is below ground level
 yr_built      | The year the house was initially built
 yr_renovated  | The year of the house's last renovation
 zipcode       | Zipcode area
 lat           | Lattitude
 long          | Longitude
 sqft_living15 | The square footage of interior housing living space for the nearest 15 neighbors
 sqft_lot15    | The square footage of the land lots of the nearest 15 neighbors

\newpage
## 2. Methods/Analysis
Firstly we will start by observing the data in the dataset then, even though the house sales dataset is already in a quite tidy form, we still can apply some data wrangling techniques to facilitate the analysis. We will split the dataset into training and validation datasets. We will get more familiar with the dataset by applying some data exploration and visualization techniques and finally we will apply some machine learning algorithms that will be measured by the RMSE metric. We will choose the algorithm with the lowest RMSE.  

### Data Observation
The dataset contains 21,613 observations and 21 features.  

```{r Data_Observation}
glimpse(data)
summary(data)
```

Let's see if there is NA's.  

```{r Data_Observation_NA}
sapply(data, function(x) sum(is.na(x)))
```

There is only one-year data, so we can't consider that we have historical data.  

```{r Data_Observation_date}
range(ymd(substring(data$date,1,8)))
```

As we could see on the dataset summary, there are some weird values for bedrooms and bathrooms (zero bedrooms or bathrooms!!!). When we look closely we can see that there is a house with 33 bedrooms in 1620 square feet and only 1.75 bathrooms, we will consider those observations as outliers and we will remove them from the dataset.  

```{r outliers}
# Houses with 0 or 33 bedrooms or 0 bathrooms 
data %>% filter( bedrooms %in% c(0,33) | bathrooms == 0 ) %>% as.tibble()

# Removing outliers
data <- data %>% filter( !(bedrooms  %in% c(0,33)) 
                        & bathrooms != 0 ) 
```

### Data Wrangling
Date feature follows the pattern yyyymmddT000000 where yyyy stands for year, mm stands for month and dd stands for the day, let's simplify the date of the sale as yyyymm with numeric class, his way we can add this feature to the correlation matrix.    

```{r wrangling_date}
data <- mutate (data, date = as.numeric(substring(data$date,1,6))) 
```

### Data Preprocessing
Let's start by picking the set of predictors, we are going to create a variable called col_index as an index of the features that will make up the set of predictors. By using the function nearZeroVar we can identify features with low variability, we will remove those features from our model. We will remove id since this feature does not provide any kind of information, finally, we also remove price from our predictors' index.  

```{r prdictors}
nzv <- nearZeroVar(data)

# Removing id, price and nzv features
colsRemoved <- c(1, 3, nzv)

# Predictors
col_index <- setdiff(1:ncol(data), colsRemoved)
```

Let's check the matrix correlation to identify predictors highly correlated with others. We add the price feature to see the correlation with the predictors set.  

```{r correlations}
# Let´s see how Price (3) is correlated with other variables, 
ggcorr(data[,c(3,col_index)], 
       name = "corr", 
       label = TRUE, 
       hjust = 1, 
       label_size = 2.5, 
       angle = -45, 
       size = 3)
```

From the correlation matrix, we can see that the main features affecting the asset price are sqft_living, grade, sqft_above, sqft_living15, and bathrooms. Interestingly, some features such as the condition or, zipcode or yr_built do not seem to have either a positive or negative correlation with the price.

The correlation matrix shows a strong correlation between sqft_living and sqft_above, so we will remove sqft_above from the predictors' set.  

```{r predictors_2}
# Removing sqft_above
colsRemoved <- c(13,colsRemoved)
col_index <- setdiff(1:ncol(data), colsRemoved)

# Let's see the predictors set
names(data)[col_index]
```

### Splitting Data
The dataset will be partitioned into two sets, the first one called train_set will be used to train the algorithms, the second one called test_set will be used to validate the algorithms. We pretend we don't know the outcome of the test_set. The validation set will be 10% of the house prices dataset.  

```{r splitting_data}
# Setting the seed of R‘s random number generator 
# in order to be able to reproduce the random objects like test_index
set.seed(1, sample.kind="Rounding")

# createDataPartition: generates indexes for randomly splitting the data 
# into training and test sets
test_index <- createDataPartition(y = data$price, times = 1, p = 0.10, list = FALSE)

train_set <- data[-test_index,]
test_set <- data[test_index,]

# checking the proportion of the test_set
nrow(test_set) / nrow(data) * 100

```

### Data Visualization
Data visualization allows us to discover relationships among dataset features. We will visualize some relationships between price and the most correlated features with the price.  

#### Price Histogram
The price histogram helps to understand how the price is distributed. The average price is 543,189 and we see that the majority of the houses' price is around the average price. Note that we use log10 transformation on the x-axis, so do not confuse with a normal distribution.  

```{r hist_price, fig.height=4, fig.width=8}
# Preventing scientific notation  
options(scipen=999)

train_set %>%
  ggplot(aes(price)) +
  geom_histogram(fill = "blue", color = "black") + 
  scale_x_continuous(trans = "log10") +
  xlab("log10(price)") 

```

#### sqft_living vs price
The below graph shows how the price increase when sqft_living increase.   

```{r sqft_living_vs_price}
train_set %>%
  ggplot(aes(sqft_living, price)) +
  geom_point(alpha = .25, color = "blue") + 
  geom_smooth(method = "lm",color="red") 

```

#### grade vs price
With the following boxplot, we can see that the higher the grade the higher the price. We will not consider grade = 3 nor 13 since there are only twelve observations, so they are not enough significant.   

```{r grade_vs_price}
# Preventing scientific notation  
options(scipen=999)

train_set %>% filter(!(grade %in% c(3, 13))) %>%
  ggplot(aes(as.factor(grade), price)) +
  geom_boxplot(fill = "blue", color = "black") + 
  scale_y_continuous(trans = "log10") +
  xlab("grade")  +
  ylab("log10(price)")

```

Let's see grade distribution in the training dataset, we can see that 7 and 8 are the most frequent grades.  
```{r grade_dist}
train_set %>%
  ggplot(aes(as.factor(grade))) +
  geom_bar(fill = "blue", color = "black") + 
  xlab("grade") 

```

#### sqft_living15 vs price
Let's see the positive correlation between sqft_living15 and price.  

```{r sqft_living15_vs_price}
# Preventing scientific notation  
options(scipen=999)

train_set %>%
  ggplot(aes(sqft_living15, price)) +
  geom_point(alpha = .25, color = "blue") + 
  geom_smooth(method = "lm",color="red")
```

#### bathrooms vs price
As you can see, in general, the more bathrooms the higher the price.
Nota that we have removed groups with less than 50 observations.  

```{r bathrooms_vs_price}
# Preventing scientific notation  
options(scipen=999)

train_set %>%
  group_by(bathrooms) %>%
  filter(n() >= 50) %>% ungroup() %>%
  ggplot(aes(as.factor(bathrooms), price)) +
  geom_boxplot(fill = "blue", color = "black") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_continuous(trans = "log10") +
  xlab("bathrooms") +
  ylab("log10(price)")
```

### Applying Machine Learning Techniques
Before starting to apply machine learning algorithms, let's define a function that computes RMSE, our goal is to build an algorithm that minimizes RMSE as much as possible.  
We always will train the algorithm by using the training set, then we will apply the algorithm on the test set and we will measure how well it fits by using the RMSE metric. for each algorithm, we will store in a dataset the algorithm's name, RMSE, percent of improvement over LSE and computation time.

```{r RMSE}
# Creating the function RMSE that computes the RMSE 
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```

#### Last Square Estimates (LSE)
This model describes the relationships among the features by using a linear relationship.
Even though we were told to go beyond standard linear regression, let's start with this model as a benchmark.

```{r model_1}
# start time
t1 <- Sys.time()

fit <- lm(price ~ ., data = train_set)

# end time
t2 <- Sys.time()

# computation time
compTime <- t2 - t1

# Let's check out all the information provided by lm function
tidy(fit, conf.int = TRUE)

# Predicting prices on test set
predicted_prices1 <- predict(fit, test_set)

# Comparing predicted prices vs actual prices
rmse1 <- RMSE(test_set$price, predicted_prices1)

# Storing the result
rmse_results <- data_frame(model = 1,
                           method = "Last Square Estimate LSE",
                           RMSE = rmse1,
                           improvement = 0,
                           time = round(compTime,2))

# Checking out results
rmse_results %>% knitr::kable()

```

Let's see if we can find an algorithm that beats this result.  

#### k-nearest neighbors
We have to pick the k that minimizes the RMSE using the training set, the function train uses cross-validation to tune k. we will tune k applying values between 7 and 61, the function predict uses the best performing model.
As cross-validation is a random procedure, we need to set the seed to make sure we can reproduce the result in the feature.

```{r model_2}
# Setting the seed
set.seed(2000)
# Cross validation trying out values between 7 and 61
# It takes around 27 minutes in my laptop !!!

# start time
t1 <- Sys.time()

train_knn <- train(train_set[,col_index], train_set[,"price"],
                   method = "knn", 
                   tuneGrid = data.frame(k = seq(7,61,2))
                   )

# end time
t2 <- Sys.time()

# computation time
compTime <- t2 - t1

# Visualizing RMSE's
ggplot(train_knn, highlight = TRUE)

# The parameter that minimizes RMSE
train_knn$bestTune
# The best performing model
train_knn$finalModel

# Predicting prices on test set
predicted_prices2 <- predict(train_knn, test_set, type = "raw")

# Comparing predicted prices vs actual prices
rmse2 <- RMSE(test_set$price, predicted_prices2)

# Storing the result
rmse_results <- bind_rows(rmse_results,
                          data_frame(model = 2,
                                     method = paste(train_knn$finalModel$k,
                                                    "-nearest neighbor regression model", 
                                                    sep = ""),
                                     RMSE = rmse2,
                                     improvement = round(100*(rmse2/rmse1 - 1),2),
                                     time = round(compTime,2)))

# Checking out results
rmse_results %>% knitr::kable()

```

The best _k_ is 19 but interestingly, we got a quite higher RMSE.

#### gamLoess
Let's try to improve by using the gamLoess method, this method has two parameters, span and degree, we set degree as 1 and try values between 0.15 and 0.95 for span.  

```{r model_3}
# Setting the seed
set.seed(2000)

# We stick degree = 1 and try ten values for span between .15 and .95
grid <- expand.grid(span = seq(0.15, 0.95, len = 10), degree = 1)

# it takes 30 minutes !!!

# start time
t1 <- Sys.time()

train_loess <- train(train_set[,col_index], train_set[,"price"],
                     method = "gamLoess",
                     tuneGrid=grid
                     ) 

# end time
t2 <- Sys.time()

# computation time
compTime <- t2 - t1

# Visualizing RMSE's
ggplot(train_loess, highlight = TRUE)

# cross validation results
train_loess$results

# The parameter that minimizes RMSE
train_loess$bestTune

# Predicting prices on test set
predicted_prices3 <- predict(train_loess, test_set)

# Comparing predicted prices vs actual prices
rmse3 <- RMSE(test_set$price, predicted_prices3)

# Storing the result
rmse_results <- bind_rows(rmse_results,
                          data_frame(model = 3,
                                     method = "gamLoess",
                                     RMSE = rmse3,
                                     improvement = round(100*(rmse3/rmse1 - 1),2),
                                     time = round(compTime,2)))

# Checking out results
rmse_results %>% knitr::kable()

```

Cross-validation picked span = 0.239, gamLoess performed better than LSE.

#### Regression Tree - rpart
rpart is a regression tree algorithm. the train function will use cross-validation to pick the complexity parameter (cp).  

```{r model 4}
# Setting the seed
set.seed(2000)

# We stick degree = 1 and try ten values for span between .15 and .95

# start time
t1 <- Sys.time()

train_rpart <- train(train_set[,col_index], train_set[,"price"],
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25))
                     )

# end time
t2 <- Sys.time()

# computation time
compTime <- t2 - t1

# Visualizing RMSE's
ggplot(train_rpart, highlight = TRUE)

# Predicting prices on test set
predicted_prices4 <- predict(train_rpart, test_set)

# Comparing predicted prices vs actual prices
rmse4 <- RMSE(test_set$price, predicted_prices4)

# Storing the result
rmse_results <- bind_rows(rmse_results,
                          data_frame(model = 4,
                                     method = "Regression Tree - rpart",
                                     RMSE = rmse4,
                                     improvement = round(100*(rmse4/rmse1 - 1),2),
                                     time = round(compTime,2)))

# Checking out results
rmse_results %>% knitr::kable()
```

rpart method performs better than LSE but worse than gamLoess. 

#### Regression Tree - randomForest 
Finally let's try randomForest method
```{r model_5}
# Setting the seed
set.seed(2000)
# it takes 3 min
# start time
t1 <- Sys.time()

train_rF <- randomForest(train_set[,col_index], train_set[,"price"])

# end time
t2 <- Sys.time()

# computation time
compTime <- t2 - t1

# Visualizing error
plot(train_rF)
train_rF
# Predicting prices on test set
predicted_prices5 <- predict(train_rF, test_set)

# Comparing predicted prices vs actual prices
rmse5 <- RMSE(test_set$price, predicted_prices5)

# Storing the result
rmse_results <- bind_rows(rmse_results,
                          data_frame(model = 5,
                                     method = "Regression Tree - randomForest",
                                     RMSE = rmse5,
                                     improvement = round(100*(rmse5/rmse1 - 1),2),
                                     time = round(compTime,2)))

# Checking out results
rmse_results %>% knitr::kable()
```

\newpage
## 3. Results
Let's evaluate the results that we got, the below table shows the results ordered by RMSE. The winner is randomForest with 21.2% of improvement over LSE and the worse is kNN with -36.2%. In spite of we tuned _k_ the knn's result is way too far from randomForest. rpart and gamLoess perform similarly, 5.9% and 8.8% respectively, taking into account the computation time that took each method we could say that rpart has a good balance between time and RMSE, randomForest performs pretty well as it runs in less than 3 minutes and improves rpart in 16.3%.  

```{r print_rmse}
# rmse_results ordered
rmse_results[order(rmse_results$RMSE),] %>% knitr::kable()
```

  
However, the winner RMSE is quite bigger than I was expecting, RMSE is a random variable that is highly impacted by large errors, let’s analyze it. Below we are going to create a data frame to compare the actual prices vs the predicted prices and the error for each prediction. 

```{r analysis_error, fig.height=4, fig.width=8}
# Preventing scientific notation  
options(scipen=999)

# a data frame with error analysis from randomForest  
df <- data_frame(y = test_set$price, 
                 y_hat = predicted_prices5,
                 diffPrice = y - y_hat)

# summary of df
df %>% summarize(avg_y = mean(y),
                 avg_y_hat = mean(y_hat),
                 minError = mean(min(abs(diffPrice))),
                 maxError = mean(max(abs(diffPrice))),
                 avgError = mean(diffPrice),
                 sdError = sd(diffPrice)
                 )

# Error histogram
df %>%
  ggplot(aes(diffPrice)) +
  geom_histogram(bins = 50, fill = "blue", color = "black") +
  scale_y_continuous(trans = "log10") +
  xlab("actual price - predicted price") + 
  ylab("log10(count)")
  
```

Not surprisingly distribution looks like a Normal, to confirm that perception let's create a QQ-plot using standard units.

```{r qq-plot}
# QQ-plot using standard units
p <- seq(0.05, 0.95, 0.05)
z <- scale(predicted_prices5)
sample_quantiles <- quantile(z, p)
theoretical_quantiles <- qnorm(p) 
qplot(theoretical_quantiles, sample_quantiles) + geom_abline()

```


## 4. Conclusion 
After training several algorithms to predict houses' prices in King County (Seattle) we came up with ramdonForest as the best one to fit our training dataset. however, an RMSE of $140,237 is way too far to be an acceptable prediction. No question, the number of observation is not enough to get good predictions, it would be great if we could get historical data and apply other machine learning algorithms to try to improve the predictions.

## 5. RStudio Version
```{r rstudio_version, echo=FALSE}
version
```


